{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f27f6ec-158e-4d61-a3e8-c1816cfc6e0f",
   "metadata": {},
   "source": [
    "# Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f26f9-604c-475d-911f-b2e30e0ab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import attr_functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from captum import attr\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as np_rand\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from xailib.explainers.lore_explainer import LoreTabularExplainer\n",
    "from xailib.models.pytorch_classifier_wrapper import pytorch_classifier_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e04921-147e-4601-b9b9-a2ce7b380e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SUBSAMPLING = True\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b52156-9332-4fb9-99bf-a45b7efa36a2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6071d25-853a-43aa-b039-095dbeab797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/Tabular/daixi_train.csv\"\n",
    "test_path = \"./data/Tabular/daixi_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cee96-73fe-43ea-9039-2d0e43521466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(train_path, sep=\";\", header=None).values\n",
    "X_test = pd.read_csv(test_path, sep=\";\", header=None)\n",
    "\n",
    "X_train = np.round(X_train, 4)\n",
    "X_test = np.round(X_test.values, 4)\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    idxs = random.sample(range(0, len(X_test)), int(len(X_test) * 0.01))\n",
    "    X_test = X_test[idxs, :]\n",
    "\n",
    "y_train = (\n",
    "    attr_functions.ssin(X_train[:, 0], X_train[:, 1], X_train[:, 2]) > 0.6\n",
    ").astype(np.float64)\n",
    "y_test = (attr_functions.ssin(X_test[:, 0], X_test[:, 1], X_test[:, 2]) > 0.6).astype(\n",
    "    np.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45134276-1484-46c0-a3e4-6425d33ef94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee51adc-7a2e-44d1-bac3-a65570c08bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Balanceig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c523d3-2c3d-4f1a-8aa0-dbf1ccf823ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "number, edges = np.histogram(y_train)\n",
    "mida = min(number)\n",
    "\n",
    "grups_x = []\n",
    "grups_y = []\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "for idx in range(len(edges) - 1):\n",
    "    selection = (y_train >= edges[idx]) & (y_train <= edges[idx + 1])\n",
    "\n",
    "    grup_y = y_train[selection]\n",
    "    grup_x = X_train[selection]\n",
    "\n",
    "    sub_select = np.random.choice(\n",
    "        np.arange(len(grup_y)), min(mida, len(grup_y)), replace=False\n",
    "    )\n",
    "\n",
    "    grups_y.append(grup_y[sub_select])\n",
    "    grups_x.append(grup_x[sub_select])\n",
    "\n",
    "X_train = np.vstack(grups_x)\n",
    "y_train = np.vstack(grups_y).flatten()\n",
    "\n",
    "p = np.random.permutation(len(y_train))\n",
    "\n",
    "X_train = X_train[p]\n",
    "y_train = y_train[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53834b16-882b-4f6a-983e-bb05c1cc08a8",
   "metadata": {},
   "source": [
    "### To tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b83afd-8b9a-486a-b69e-d94497d9d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df = X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_df)\n",
    "X_train_df.columns = [\"x1\", \"x2\", \"x3\"]\n",
    "X_train_df[\"target\"] = y_train.flatten()\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_df)\n",
    "X_test_df.columns = [\"x1\", \"x2\", \"x3\"]\n",
    "X_test_df[\"target\"] = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826cc93a-a562-4860-8a92-27b1675aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a93f7a-8098-4b45-b618-359e80bc5c49",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d91943-ace0-4ab2-a68c-de20b7ee3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ftrs = X_train.shape[1]\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_ftrs, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 1),\n",
    "    # nn.Sigmoid()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15ee8d-cac7-410d-a9bb-13fa09d5f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1ff22-f7cf-4e89-99e9-b7396216beda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f4188-368a-446e-af7e-fb9a398e630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metric(a, b):\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.cpu().numpy()\n",
    "\n",
    "    if isinstance(b, torch.Tensor):\n",
    "        b = b.cpu().numpy()\n",
    "\n",
    "    return mae(a, b)\n",
    "\n",
    "\n",
    "def cls_metric(a, b):\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.cpu().numpy()\n",
    "\n",
    "    if isinstance(b, torch.Tensor):\n",
    "        b = b.cpu().numpy()\n",
    "\n",
    "    return accuracy_score(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f775a-7cd8-435e-a756-04d678813f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "LR = 0.0001\n",
    "GAMMA = 0.85\n",
    "STEP_SIZE = 150\n",
    "\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "pbar = tqdm(range(EPOCHS), desc=\"Time, he's waiting in the wings\")\n",
    "\n",
    "best_val = 0\n",
    "best_model = None\n",
    "\n",
    "wandb.init(\n",
    "    project=\"daixi\",\n",
    "    config={\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"step\": STEP_SIZE,\n",
    "        \"gamma\": GAMMA,\n",
    "    },\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Create the scheduler after the optimizer\n",
    "scheduler = StepLR(\n",
    "    optimizer, step_size=STEP_SIZE, gamma=GAMMA\n",
    ")  # adjust step_size and gamma as needed\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "for epoch in pbar:\n",
    "    net.train()\n",
    "    output = net(X_train.to(device))\n",
    "    loss = criterion(output, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    output = net(X_test.to(device)).cpu().detach().numpy()\n",
    "    res_val = cls_metric((output > 0), y_test)\n",
    "\n",
    "    wandb.log({\"Train MAE\": loss, \"Val MAE\": res_val})\n",
    "\n",
    "    if res_val > best_val:\n",
    "        best_val = res_val\n",
    "        best_model = copy.deepcopy(net.state_dict())\n",
    "\n",
    "    scheduler.step()  # update learning rate\n",
    "    pbar.set_description(\n",
    "        f\"Epoch {epoch}/{EPOCHS} - Val. Loss {round(loss.item(), 2)} - Val. Perform.: {round(res_val, 2)}\"\n",
    "    )\n",
    "\n",
    "net.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91dc51-5242-4e01-ae41-147657242d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(best_model)\n",
    "net.eval()\n",
    "print(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d80da-187d-40d1-82a2-02e800fcd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./output/ssin_cls_tabular.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679347c-372c-4a0e-b9e3-1459c6a9b56e",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5de676-27b8-42cf-b3e4-b585cfe83ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./output/ssin_cls_tabular.pt\", weights_only=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e03ff-ee7a-46f0-8b46-b70b7167c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = net(X_test.to(device)) > 0\n",
    "\n",
    "print(classification_report(y_test.cpu().numpy(), pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5d666-e268-430a-823b-cee45a514f6a",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cf51f-51e2-4aab-8ce1-9dddb35b2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def _to_probability(info):\n",
    "    \"\"\"Convert the input to a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        info: NumPy array with the input to convert.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the input converted to a probability distribution\n",
    "    \"\"\"\n",
    "    if isinstance(info, torch.Tensor):\n",
    "        info = info.cpu().detach().numpy()\n",
    "    info = np.copy(info)\n",
    "    info_shape = info.shape\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    info = info.reshape(-1, 1)\n",
    "    info = scaler.fit_transform(info)\n",
    "    info = info.reshape(info_shape)\n",
    "\n",
    "    return info / (np.sum(info) + epsilon)\n",
    "\n",
    "\n",
    "def kl(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the Kullback-Leibler divergence between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    sal_map_gt = sal_map_gt + epsilon\n",
    "    sal_map = sal_map + epsilon\n",
    "\n",
    "    divergence = np.sum(sal_map_gt * np.log(sal_map_gt / sal_map))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "\n",
    "def emd(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Earth Mover's Distance between two saliency maps.\n",
    "\n",
    "    Earth Mover's Distance (EMD) is a measure of the distance between two probability distributions over a region.\n",
    "    It is defined as the minimum cost of turning one distribution into the other, where the cost is the amount of\n",
    "    \"earth\" moved, or the amount of probability mass that must be moved from one point to another.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float between 0 and 1 with the EMD between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    sal_map_gt /= sal_map_gt.max() if sal_map_gt.max() > 0 else 1\n",
    "    sal_map /= sal_map.max() if sal_map.max() > 0 else 1\n",
    "\n",
    "    diff = stats.wasserstein_distance(sal_map.flatten(), sal_map_gt.flatten())\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def _to_zero_one(info):\n",
    "    return (info - info.min()) / (info.max() - info.min())\n",
    "\n",
    "\n",
    "def AUC_Borji(sal_map_gt, sal_map, n_rep=100, step_size=0.1, rand_sampler=None):\n",
    "    \"\"\"\n",
    "    This measures how well the saliency map of an image predicts the ground truth human fixations on the image.\n",
    "    ROC curve created by sweeping through threshold values at fixed step size until the maximum saliency map value.\n",
    "        - True positive (tp) rate correspond to the ratio of saliency map values above threshold\n",
    "          at fixation locations to the total number of fixation locations.\n",
    "        - False positive (fp) rate correspond to the ratio of saliency map values above threshold\n",
    "          at random locations to the total number of random locations (as many random locations as fixations,\n",
    "            sampled uniformly from fixation_map ALL IMAGE PIXELS), averaging over n_rep number of selections of random locations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    saliency_map : real-valued matrix\n",
    "    fixation_map : binary matrix\n",
    "        Human fixation map.\n",
    "    n_rep : int, optional\n",
    "        Number of repeats for random sampling of non-fixated locations.\n",
    "    step_size : int, optional\n",
    "        Step size for sweeping through saliency map.\n",
    "    rand_sampler : callable\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "        Sample the saliency map at random locations to estimate false positive.\n",
    "        Return the sampled saliency values, S_rand.shape=(n_fix,n_rep)\n",
    "    Returns\n",
    "    -------\n",
    "    AUC : float, between [0,1]\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_zero_one(sal_map_gt)\n",
    "    sal_map = _to_zero_one(sal_map)\n",
    "\n",
    "    saliency_map = np.asarray(sal_map)\n",
    "    fixation_map = np.asarray(sal_map_gt) > 0.5\n",
    "    # If there are no fixation to predict, return NaN\n",
    "    if not np.any(fixation_map):\n",
    "        print(\"no fixation to predict\")\n",
    "        return np.nan\n",
    "    # # Normalize saliency map to have values between [0,1]\n",
    "    # saliency_map = _to_probability(saliency_map)\n",
    "\n",
    "    S = saliency_map.ravel()\n",
    "    F = fixation_map.ravel()\n",
    "    S_fix = S[F]  # Saliency map values at fixation locations\n",
    "    n_fix = len(S_fix)\n",
    "    n_pixels = len(S)\n",
    "    # For each fixation, sample n_rep values from anywhere on the saliency map\n",
    "    if rand_sampler is None:\n",
    "        r = np_rand.randint(0, n_pixels, [n_fix, n_rep])\n",
    "        S_rand = S[r]  # Saliency map values at random locations (including fixated locations!? underestimated)\n",
    "    else:\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "    # Calculate AUC per random split (set of random locations)\n",
    "    auc = np.zeros(n_rep) * np.nan\n",
    "    for rep in range(n_rep):\n",
    "        thresholds = np.r_[0 : np.max(np.r_[S_fix, S_rand[:, rep]]) : step_size][::-1]\n",
    "        tp = np.zeros(len(thresholds) + 2)\n",
    "        fp = np.zeros(len(thresholds) + 2)\n",
    "        tp[0] = 0\n",
    "        tp[-1] = 1\n",
    "        fp[0] = 0\n",
    "        fp[-1] = 1\n",
    "        for k, thresh in enumerate(thresholds):\n",
    "            tp[k + 1] = np.sum(S_fix >= thresh) / float(n_fix)\n",
    "            fp[k + 1] = np.sum(S_rand[:, rep] >= thresh) / float(n_fix)\n",
    "        auc[rep] = np.trapz(tp, fp)\n",
    "    return np.mean(auc)  # Average across random splits\n",
    "\n",
    "\n",
    "def sim(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the sim distance between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the min distance between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # sal_map_gt /= np.sum(sal_map_gt) + epsilon\n",
    "    # sal_map /= np.sum(sal_map) + epsilon\n",
    "\n",
    "    diff = np.min(np.stack([sal_map, sal_map_gt]), axis=0)\n",
    "    diff = np.sum(diff)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    # \"emd\": emd,\n",
    "    \"kl\": kl,\n",
    "    \"auc\": AUC_Borji,\n",
    "    \"sim\": sim,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1f0a-84dd-4cd7-a5ee-fb6c84ba91f3",
   "metadata": {},
   "source": [
    "## Get GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446f00-c369-4158-a7e4-617cc9f1fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # for x in tqdm(X_test):\n",
    "    org_output = net(X_test.to(device)).cpu().detach()\n",
    "    importance = torch.zeros_like(X_test)\n",
    "\n",
    "    for i in range(3):\n",
    "        x_prime = X_test.clone()\n",
    "        x_prime[:, i] = 0\n",
    "        aux_output = net(x_prime.to(device)).cpu().detach()\n",
    "\n",
    "        importance[:, i] = (org_output - aux_output)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104ef1c-888b-4cd4-9e6a-a3384962e878",
   "metadata": {},
   "source": [
    "## XAI Methods\n",
    "\n",
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036a8ea-57b1-4c14-8450-176a9ba17edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(explainer, data):\n",
    "    org_shape = data.shape\n",
    "\n",
    "    if len(data.shape) > 1:\n",
    "        data = data.flatten()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            data = data.detach().cpu().numpy()\n",
    "        elif not isinstance(data, np.array):\n",
    "            raise Exception(\n",
    "                f\"Input data must be either a Pytorch Tensor or a Numpy array, instead {type(data)}.\"\n",
    "            )\n",
    "        exp = explainer.explain_instance(\n",
    "            data,\n",
    "            lambda x: net(torch.Tensor(x).to(device)).cpu().numpy(),\n",
    "            num_features=3,\n",
    "            top_labels=1,\n",
    "        )\n",
    "\n",
    "        expl = sorted(list(exp.as_map().values())[0], key=lambda x: x[0])\n",
    "        expl = torch.Tensor([float(e[1]) for e in expl])\n",
    "        expl = expl.reshape(org_shape)\n",
    "\n",
    "        return expl\n",
    "\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.cpu().numpy(), discretize_continuous=True, mode=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21f2b1-38dd-4db0-a06b-dc01d77f553d",
   "metadata": {},
   "source": [
    "### Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f26fb-304a-48c4-af50-3222242ea705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, xai):\n",
    "    res = xai.attribute(x.to(device), target=0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "sal = attr.Saliency(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc705467-2993-4448-8222-21f114d214a7",
   "metadata": {},
   "source": [
    "### DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7ef37-6fe4-4674-a658-a5871c84e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift = attr.DeepLift(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957763b-b877-457f-afa6-6d9d55d7a582",
   "metadata": {},
   "source": [
    "### IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16b92-0373-4995-9fea-064b114c4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = attr.IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77791dd-df93-4c40-a8c1-950808de760d",
   "metadata": {},
   "source": [
    "### LORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe6947-4ab4-4c37-8190-37676188825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = pytorch_classifier_wrapper(net, device=device, n_features=3)\n",
    "explainer_lore = LoreTabularExplainer(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c399c2-254c-4f4b-a2b5-78940ada671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"neigh_type\": \"geneticp\", \"size\": 1000, \"ocr\": 0.1, \"ngen\": 10}\n",
    "explainer_lore.fit(X_train_df, \"target\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aebb69-01c7-4343-99f4-021025bd5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_dt(estimator, instance):\n",
    "    \"\"\"Attribute the importance of each feature in the prediction of a decision tree.\n",
    "\n",
    "    Args:\n",
    "        estimator: (sklearn.model). Decision tree model.\n",
    "        instance: (np.array) Instance to explain.\n",
    "\n",
    "    Returns:\n",
    "        np.array with the attribution of each feature.\n",
    "    \"\"\"\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "    impurity = estimator.tree_.impurity\n",
    "\n",
    "    importance = {}\n",
    "\n",
    "    node_id = 0\n",
    "    while children_left[node_id] != children_right[node_id]:\n",
    "\n",
    "        if feature[node_id] not in importance:\n",
    "            importance[feature[node_id]] = 0\n",
    "\n",
    "        if instance[feature[node_id]] <= threshold[node_id]:\n",
    "            children_id = children_left[node_id]\n",
    "        else:\n",
    "            children_id = children_right[node_id]\n",
    "\n",
    "        importance[feature[node_id]] += impurity[node_id] - impurity[children_id]\n",
    "        node_id = children_id\n",
    "\n",
    "    attribution = np.zeros_like(instance).astype(np.float64)\n",
    "\n",
    "    adder = 0\n",
    "    for feature, value in importance.items():\n",
    "        adder += value\n",
    "        attribution[feature] = value\n",
    "\n",
    "    attribution /= adder\n",
    "\n",
    "    return attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115d79b-8f89-4031-8114-3dd10c3b8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lore(inst):\n",
    "    if isinstance(inst, torch.Tensor):\n",
    "        inst = inst.cpu().detach().numpy()\n",
    "\n",
    "    inst = inst.flatten()\n",
    "    exp = explainer_lore.explain(inst)\n",
    "\n",
    "    return attribute_dt(exp.exp.dt, inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf9d20-74d1-4ee4-b7bc-2b74a0b08d40",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14998c0-3171-4b5a-afe6-ae19fd00056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4e167-b7ee-4ad3-83f2-ebd384d5301f",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01b368-0353-4881-9c2a-4728dafe27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"lime\": lambda x: get_lime(explainer, x),\n",
    "    \"grad\": lambda x: grad_fn(x, sal),\n",
    "    \"deep_lift\": lambda x: grad_fn(x, deep_lift),\n",
    "    \"shap\": lambda x: kernel_shap.attribute(x.to(device), target=0, n_samples=200),\n",
    "    \"lore\": get_lore,\n",
    "    \"ig\": lambda x: grad_fn(x, ig),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9037de7-9bd8-4a87-861e-1f9ef08e0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results_tabular_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25720771-1d92-4fb5-8bfe-2ec2a0d46b97",
   "metadata": {},
   "source": [
    "#### Obtain the explanations for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aebc32-4c15-46b7-8f2c-c005914a8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, method in methods.items():\n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "\n",
    "    raw_results = []\n",
    "    for x, y, gt in zip(tqdm(X_test, desc=method_name), y_test, importance):\n",
    "        explanation = method(x.reshape(1, 3))\n",
    "        if isinstance(explanation, torch.Tensor):\n",
    "            explanation = explanation.cpu().detach().numpy()\n",
    "\n",
    "        raw_results.append(explanation)\n",
    "    raw_results = np.vstack(raw_results)\n",
    "\n",
    "    with open(os.path.join(\"results\", \"tabular\", f\"{method_name}.npy\"), \"wb\") as f:\n",
    "        np.save(f, raw_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262a2b2-97cd-44b7-92fe-933187cbebb9",
   "metadata": {},
   "source": [
    "#### Obtain the metrics for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3854e0-dd63-4574-a893-9314384bb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for method_name in methods.keys():\n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "\n",
    "    raw_results = np.load(os.path.join(\"results\", \"tabular\", f\"{method_name}.npy\"))\n",
    "\n",
    "    for explanation, y, gt, gt_xai in zip(\n",
    "        tqdm(raw_results, desc=method_name), pred, y_test, importance\n",
    "    ):\n",
    "        if int(y.detach().cpu().numpy()) != int(gt):\n",
    "            continue\n",
    "\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            res = metric_fn(gt_xai.flatten(), explanation.flatten())\n",
    "            results_method[metric_name].append(float(res))\n",
    "\n",
    "    results[method_name] = results_method\n",
    "\n",
    "with open(RESULTS_PATH, \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976d0b0-2070-4dcc-98b7-eaff3c69197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for method_name, method_info in results.items():\n",
    "    print(method_name.upper())\n",
    "    for k, v in method_info.items():\n",
    "        print(f\"{k}: {np.nanmean(v)} - {np.nanstd(v)}\")\n",
    "    print(\"-\" * 25)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python (uv Auto-sync)",
   "language": "python",
   "name": "auto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
