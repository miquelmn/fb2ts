{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f27f6ec-158e-4d61-a3e8-c1816cfc6e0f",
   "metadata": {},
   "source": [
    "# Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f26f9-604c-475d-911f-b2e30e0ab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from captum import attr\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "from xailib.models.pytorch_classifier_wrapper import pytorch_classifier_wrapper\n",
    "from xailib.explainers.lore_explainer import LoreTabularExplainer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "\n",
    "import attr_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e04921-147e-4601-b9b9-a2ce7b380e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SUBSAMPLING = True\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b52156-9332-4fb9-99bf-a45b7efa36a2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6071d25-853a-43aa-b039-095dbeab797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/Tabular/daixi_train.csv\"\n",
    "test_path = \"./data/Tabular/daixi_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cee96-73fe-43ea-9039-2d0e43521466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(train_path, sep=\";\", header=None).values\n",
    "X_test = pd.read_csv(test_path, sep=\";\", header=None)\n",
    "\n",
    "X_train = np.round(X_train, 4)\n",
    "X_test = np.round(X_test.values, 4)\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    idxs = random.sample(range(0, len(X_test)), int(len(X_test) * 0.01))\n",
    "    X_test = X_test[idxs, :]\n",
    "\n",
    "y_train = (attr_functions.ssin(X_train[:, 0], X_train[:, 1], X_train[:, 2]) > 0.6).astype(np.float64)\n",
    "y_test = (attr_functions.ssin(X_test[:, 0], X_test[:, 1], X_test[:, 2]) > 0.6).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45134276-1484-46c0-a3e4-6425d33ef94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee51adc-7a2e-44d1-bac3-a65570c08bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Balanceig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c523d3-2c3d-4f1a-8aa0-dbf1ccf823ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "number, edges = np.histogram(y_train)\n",
    "mida = min(number)\n",
    "\n",
    "grups_x = []\n",
    "grups_y = []\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "for idx in range(len(edges) - 1):\n",
    "    selection = (y_train >= edges[idx]) & (y_train <= edges[idx + 1])\n",
    "    \n",
    "    grup_y = y_train[selection]\n",
    "    grup_x = X_train[selection]\n",
    "\n",
    "    sub_select = np.random.choice(np.arange(len(grup_y)), min(mida, len(grup_y)), replace=False)\n",
    "\n",
    "    grups_y.append(grup_y[sub_select])\n",
    "    grups_x.append(grup_x[sub_select])\n",
    "\n",
    "X_train = np.vstack(grups_x)\n",
    "y_train = np.vstack(grups_y).flatten()\n",
    "\n",
    "p = np.random.permutation(len(y_train))\n",
    "\n",
    "X_train = X_train[p]\n",
    "y_train = y_train[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53834b16-882b-4f6a-983e-bb05c1cc08a8",
   "metadata": {},
   "source": [
    "### To tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b83afd-8b9a-486a-b69e-d94497d9d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df = X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_df)\n",
    "X_train_df.columns = [\"x1\", \"x2\", \"x3\"]\n",
    "X_train_df[\"target\"] = y_train.flatten()\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_df)\n",
    "X_test_df.columns = [\"x1\", \"x2\", \"x3\"]\n",
    "X_test_df[\"target\"] = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826cc93a-a562-4860-8a92-27b1675aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a93f7a-8098-4b45-b618-359e80bc5c49",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d91943-ace0-4ab2-a68c-de20b7ee3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ftrs = X_train.shape[1]\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(input_ftrs, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 1),\n",
    "    # nn.Sigmoid()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15ee8d-cac7-410d-a9bb-13fa09d5f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1ff22-f7cf-4e89-99e9-b7396216beda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f4188-368a-446e-af7e-fb9a398e630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metric(a, b):\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.cpu().numpy()\n",
    "\n",
    "    if isinstance(b, torch.Tensor):\n",
    "        b = b.cpu().numpy()\n",
    "        \n",
    "    return mae(a, b)\n",
    "\n",
    "def cls_metric(a, b):\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.cpu().numpy()\n",
    "\n",
    "    if isinstance(b, torch.Tensor):\n",
    "        b = b.cpu().numpy()\n",
    "    \n",
    "    return accuracy_score(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f775a-7cd8-435e-a756-04d678813f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "LR = 0.0001\n",
    "GAMMA = 0.85\n",
    "STEP_SIZE = 150\n",
    "\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "pbar = tqdm(range(EPOCHS), desc='Time, he\\'s waiting in the wings')\n",
    "\n",
    "best_val = 0\n",
    "best_model = None\n",
    "\n",
    "wandb.init(project=\"daixi\", config={\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LR,\n",
    "    \"step\": STEP_SIZE,\n",
    "    \"gamma\": GAMMA,\n",
    "})\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Create the scheduler after the optimizer\n",
    "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)  # adjust step_size and gamma as needed\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "for epoch in pbar:\n",
    "    net.train()\n",
    "    output = net(X_train.to(device))\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.eval()\n",
    "    output = net(X_test.to(device)).cpu().detach().numpy()\n",
    "    res_val = cls_metric((output > 0), y_test)\n",
    "    \n",
    "    wandb.log({\"Train MAE\": loss, \"Val MAE\": res_val})\n",
    "\n",
    "    if res_val > best_val:\n",
    "        best_val = res_val\n",
    "        best_model = copy.deepcopy(net.state_dict())\n",
    "    \n",
    "    scheduler.step()  # update learning rate\n",
    "    pbar.set_description(f'Epoch {epoch}/{EPOCHS} - Val. Loss {round(loss.item(), 2)} - Val. Perform.: {round(res_val, 2)}')\n",
    "\n",
    "net.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91dc51-5242-4e01-ae41-147657242d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(best_model)\n",
    "net.eval()\n",
    "print(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d80da-187d-40d1-82a2-02e800fcd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./output/ssin_cls_tabular.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679347c-372c-4a0e-b9e3-1459c6a9b56e",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5de676-27b8-42cf-b3e4-b585cfe83ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"./output/psin_cls_tabular.pt\", weights_only=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5d666-e268-430a-823b-cee45a514f6a",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cf51f-51e2-4aab-8ce1-9dddb35b2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def _to_probability(info):\n",
    "    \"\"\"Convert the input to a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        info: NumPy array with the input to convert.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the input converted to a probability distribution\n",
    "    \"\"\"\n",
    "    if isinstance(info, torch.Tensor):\n",
    "        info = info.cpu().detach().numpy()\n",
    "    info = np.copy(info)\n",
    "    info_shape = info.shape\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    info = info.reshape(-1, 1)\n",
    "    info = scaler.fit_transform(info)\n",
    "    info = info.reshape(info_shape)\n",
    "\n",
    "    return info / (np.sum(info) + epsilon)\n",
    "\n",
    "\n",
    "def kl(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the Kullback-Leibler divergence between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    sal_map_gt = sal_map_gt + epsilon\n",
    "    sal_map = sal_map + epsilon\n",
    "\n",
    "    divergence = np.sum(sal_map_gt * np.log(sal_map_gt / sal_map))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "def emd(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Earth Mover's Distance between two saliency maps.\n",
    "\n",
    "    Earth Mover's Distance (EMD) is a measure of the distance between two probability distributions over a region.\n",
    "    It is defined as the minimum cost of turning one distribution into the other, where the cost is the amount of\n",
    "    \"earth\" moved, or the amount of probability mass that must be moved from one point to another.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float between 0 and 1 with the EMD between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    sal_map_gt /= sal_map_gt.max() if sal_map_gt.max() > 0 else 1\n",
    "    sal_map /= sal_map.max() if sal_map.max() > 0 else 1\n",
    "\n",
    "    diff = stats.wasserstein_distance(sal_map.flatten(), sal_map_gt.flatten())\n",
    "\n",
    "    return diff\n",
    "\n",
    "metrics = {\n",
    "    \"emd\": emd,\n",
    "    \"kl\": kl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1f0a-84dd-4cd7-a5ee-fb6c84ba91f3",
   "metadata": {},
   "source": [
    "## Get GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446f00-c369-4158-a7e4-617cc9f1fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # for x in tqdm(X_test):\n",
    "    org_output = net(X_test.to(device)).cpu().detach()\n",
    "    importance = torch.zeros_like(X_test)\n",
    "\n",
    "    for i, val in enumerate([0, 0, 0]):\n",
    "        x_prime = X_test.clone()\n",
    "        x_prime[:, i] = 0\n",
    "        aux_output = net(x_prime.to(device)).cpu().detach()\n",
    "\n",
    "        importance[:, i] = (org_output - aux_output)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104ef1c-888b-4cd4-9e6a-a3384962e878",
   "metadata": {},
   "source": [
    "## XAI Methods\n",
    "\n",
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036a8ea-57b1-4c14-8450-176a9ba17edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(explainer, data):\n",
    "    org_shape = data.shape\n",
    "    \n",
    "    if len(data.shape) > 1:\n",
    "        data = data.flatten()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            data = data.detach().cpu().numpy()\n",
    "        elif not isinstance(data, np.array):\n",
    "            raise Exception(f\"Input data must be either a Pytorch Tensor or a Numpy array, instead {type(data)}.\")\n",
    "        exp = explainer.explain_instance(\n",
    "            data, \n",
    "            lambda x: net(torch.Tensor(x).to(device)).cpu().numpy(), \n",
    "            num_features=3, \n",
    "            top_labels=1\n",
    "        )\n",
    "    \n",
    "        expl = sorted(list(exp.as_map().values())[0], key = lambda x: x[0])\n",
    "        expl = torch.Tensor([float(e[1]) for e in expl])\n",
    "        expl = expl.reshape(org_shape)\n",
    "        \n",
    "        return expl\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.cpu().numpy(), \n",
    "    discretize_continuous=True,\n",
    "    mode=\"regression\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21f2b1-38dd-4db0-a06b-dc01d77f553d",
   "metadata": {},
   "source": [
    "### Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f26fb-304a-48c4-af50-3222242ea705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, xai):    \n",
    "    res = xai.attribute(x.to(device), target=0)\n",
    "    \n",
    "    return res\n",
    "\n",
    "sal = attr.Saliency(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc705467-2993-4448-8222-21f114d214a7",
   "metadata": {},
   "source": [
    "### DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7ef37-6fe4-4674-a658-a5871c84e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift = attr.DeepLift(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957763b-b877-457f-afa6-6d9d55d7a582",
   "metadata": {},
   "source": [
    "### IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab16b92-0373-4995-9fea-064b114c4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = attr.IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77791dd-df93-4c40-a8c1-950808de760d",
   "metadata": {},
   "source": [
    "### LORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe6947-4ab4-4c37-8190-37676188825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = pytorch_classifier_wrapper(net, device=device)\n",
    "explainer_lore = LoreTabularExplainer(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c399c2-254c-4f4b-a2b5-78940ada671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'neigh_type':'geneticp', 'size': 1000, 'ocr':0.1, 'ngen':10}\n",
    "explainer_lore.fit(X_train_df, 'target', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aebb69-01c7-4343-99f4-021025bd5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_dt(estimator, instance):\n",
    "    \"\"\" Attribute the importance of each feature in the prediction of a decision tree.\n",
    "\n",
    "    Args:\n",
    "        estimator: (sklearn.model). Decision tree model.\n",
    "        instance: (np.array) Instance to explain.\n",
    "\n",
    "    Returns:\n",
    "        np.array with the attribution of each feature.\n",
    "    \"\"\"\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "    impurity = estimator.tree_.impurity\n",
    "\n",
    "    importance = {}\n",
    "\n",
    "    node_id = 0\n",
    "    while children_left[node_id] != children_right[node_id]:\n",
    "\n",
    "        if feature[node_id] not in importance:\n",
    "            importance[feature[node_id]] = 0\n",
    "\n",
    "        if instance[feature[node_id]] <= threshold[node_id]:\n",
    "            children_id = children_left[node_id]\n",
    "        else:\n",
    "            children_id = children_right[node_id]\n",
    "\n",
    "        importance[feature[node_id]] += (impurity[node_id] - impurity[children_id])\n",
    "        node_id = children_id\n",
    "\n",
    "    attribution = np.zeros_like(instance).astype(np.float64)\n",
    "\n",
    "    adder = 0\n",
    "    for feature, value in importance.items():\n",
    "        adder += value\n",
    "        attribution[feature] = value\n",
    "\n",
    "    attribution /= adder\n",
    "\n",
    "    return attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115d79b-8f89-4031-8114-3dd10c3b8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lore(inst):\n",
    "    if isinstance(inst, torch.Tensor):\n",
    "        inst = inst.cpu().detach().numpy()\n",
    "        \n",
    "    inst = inst.flatten()\n",
    "    exp = explainer_lore.explain(inst)\n",
    "    \n",
    "    return attribute_dt(exp.exp.dt, inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf9d20-74d1-4ee4-b7bc-2b74a0b08d40",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14998c0-3171-4b5a-afe6-ae19fd00056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01b368-0353-4881-9c2a-4728dafe27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"lime\": lambda x: get_lime(explainer, x),\n",
    "    \"grad\": lambda x: grad_fn(x, sal),\n",
    "    \"deep_lift\": lambda x: grad_fn(x, deep_lift),\n",
    "    \"shap\": lambda x: kernel_shap.attribute(x.to(device), target=0, n_samples=200),\n",
    "    \"lore\": get_lore,\n",
    "    \"ig\": lambda x: grad_fn(x, ig)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4e167-b7ee-4ad3-83f2-ebd384d5301f",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cec07d-6d75-44bc-b59b-47af9d26bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "RESULTS_PATH = \"./results_psin_cls.json\"\n",
    "results = dict()\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "\n",
    "    for x, gt in zip(tqdm(X_test, desc=method_name), importance):\n",
    "        explanation = method(x.reshape(1, 3))\n",
    "\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            res = metric_fn(gt, explanation)\n",
    "            results_method[metric_name].append(float(res))\n",
    "    results[method_name] = results_method\n",
    "\n",
    "    with open(RESULTS_PATH, \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976d0b0-2070-4dcc-98b7-eaff3c69197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, method_info in results.items():\n",
    "    print(method_name.upper())\n",
    "    for k, v in method_info.items():\n",
    "        print(f\"{k}: {np.nanmean(v)} - {np.nanstd(v)}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78d2e7-98ab-4717-92bf-60e74a98965f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "venv_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
