{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae1772-464c-45f3-9808-2e6940bacaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import rise\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fe08d-6f93-4136-ae12-fd5504a91830",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"./data/funny_birds/v2/test_CO/**/**/image.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c4f9f-6542-41e1-a598-eab0677c435d",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "ResNET50 with a different head. Accuracy over ~0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefa635-d02f-499b-9f0c-1a3cda973e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = models.resnet50(num_classes = 50)\n",
    "\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                           bias=False)\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.7),\n",
    "    nn.Linear(128, 47),\n",
    ")\n",
    "\n",
    "\n",
    "net.load_state_dict(torch.load('res_9.pt', weights_only=True))\n",
    "net = net.to(device)\n",
    "\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7225b-3970-4734-9c58-b47d68e84fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Grayscale(),\n",
    "    v2.ToDtype(torch.float),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7668fc-627f-4448-9046-d26c0567454e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get GT Relevance\n",
    "\n",
    "Obtains the relevance of each part as:\n",
    "\n",
    "$$\n",
    "\\text{Part}_{\\text{imp}} = f(x) - f(x'),\n",
    "$$\n",
    "\n",
    "where $x'$ is the same image than $x$ without the part studied.\n",
    "\n",
    "This $\\text{Part}_{\\text{imp}}$ is multiplied by the respective part. The addition of all part multiplied with their importance generates a saliency map GT, that is stored as a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29e455-1a5b-4310-9de8-368de6a3920e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for img_path in tqdm(sorted(glob(DATA_PATH))):\n",
    "    folder_path, img_name = os.path.split(img_path)\n",
    "    cls = folder_path.split(os.path.sep)[-2]\n",
    "    img_id = folder_path.split(os.path.sep)[-1]\n",
    "    \n",
    "    original_data = Image.open(img_path)\n",
    "\n",
    "    # Prepare the data for the model\n",
    "    original_data = transform(original_data).unsqueeze(0)\n",
    "    \n",
    "    output_org = net(original_data.to(device))[0][int(cls)] # [batch][class]\n",
    "    \n",
    "    parts = {\n",
    "        \"wing\": \"body_beak_eye_foot_tail.png\", \n",
    "        \"tail\": \"body_beak_eye_foot_wing.png\",\n",
    "        \"foot\": \"body_beak_eye_tail_wing.png\",\n",
    "        \"eye\": \"body_beak_foot_tail_wing.png\",\n",
    "        \"beak\": \"body_eye_foot_tail_wing.png\",\n",
    "    }\n",
    "\n",
    "    sal_map_gt = torch.zeros_like(original_data).float()\n",
    "    res = dict()\n",
    "    diff_arr = []\n",
    "    for part, part_path in parts.items():\n",
    "        path = os.path.join(folder_path, part_path)\n",
    "        \n",
    "        if not os.path.isfile(path):\n",
    "            print(f\"{path} - fora\")\n",
    "        \n",
    "        data = Image.open(path)\n",
    "        data =  transform(data).unsqueeze(0) # transforms.ToTensor()(data).unsqueeze(0)\n",
    "\n",
    "        output = net(data.to(device))[0][int(cls)] # [batch][class]\n",
    "        res[part] = float((output_org - output).detach().cpu())\n",
    "\n",
    "        diff = original_data - data\n",
    "        diff = diff * res[part]\n",
    "\n",
    "        diff_arr.append(diff)\n",
    "\n",
    "        sal_map_gt = sal_map_gt + diff\n",
    "    sal_map_gt = sal_map_gt.cpu().numpy()[0, 0, :, :]\n",
    "    with open(f\"./output/GT_resnet50/{img_id}.npy\", 'wb') as f:\n",
    "        np.save(f, sal_map_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753aa30-7d80-4665-9ce4-8d5e78913e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i, a in enumerate(diff_arr):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(abs(a.cpu().numpy()[0, 0, :, :]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d9ae7-c753-46d2-a14a-8d5d806d7dfa",
   "metadata": {},
   "source": [
    "# Get XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae0c3f-b087-40d7-a4e8-5d678fc22d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def _to_probability(info):\n",
    "    \"\"\"Convert the input to a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        info: NumPy array with the input to convert.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the input converted to a probability distribution\n",
    "    \"\"\"\n",
    "    info = np.copy(info)\n",
    "    info_shape = info.shape\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    info = info.reshape(-1, 1)\n",
    "    info = scaler.fit_transform(info)\n",
    "    info = info.reshape(info_shape)\n",
    "\n",
    "    return info / (np.sum(info) + epsilon)\n",
    "\n",
    "\n",
    "def kl(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the Kullback-Leibler divergence between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    sal_map_gt = sal_map_gt + epsilon\n",
    "    sal_map = sal_map + epsilon\n",
    "\n",
    "    divergence = np.sum(sal_map_gt * np.log(sal_map_gt / sal_map))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "def emd(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Earth Mover's Distance between two saliency maps.\n",
    "\n",
    "    Earth Mover's Distance (EMD) is a measure of the distance between two probability distributions over a region.\n",
    "    It is defined as the minimum cost of turning one distribution into the other, where the cost is the amount of\n",
    "    \"earth\" moved, or the amount of probability mass that must be moved from one point to another.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float between 0 and 1 with the EMD between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    sal_map_gt /= sal_map_gt.max() if sal_map_gt.max() > 0 else 1\n",
    "    sal_map /= sal_map.max() if sal_map.max() > 0 else 1\n",
    "\n",
    "    diff = stats.wasserstein_distance(sal_map.flatten(), sal_map_gt.flatten())\n",
    "\n",
    "    return diff\n",
    "\n",
    "metrics = {\n",
    "    \"emd\": emd,\n",
    "    \"kl\": kl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e7a1d-d761-4aa1-839e-08554e018b41",
   "metadata": {},
   "source": [
    "## RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323f8c0-64a9-40d6-8cd1-ecf0d42b223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_rise = rise.RISE(net, (256, 256), gpu_batch=1, device=device)\n",
    "explainer_rise.generate_masks(N=600, s=8, p1=0.1, savepath=\"masks.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb77b70-50ab-4cbd-9b0a-dc8850239657",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544582b-f248-4085-a395-7b24df0965f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_image import LimeImageExplainer\n",
    "\n",
    "explainer_lime = LimeImageExplainer()\n",
    "\n",
    "def batch_predict(image: np.array, network, multi_channel: bool = False, n_classes = None) -> np.array:\n",
    "    \"\"\"Function to predict the output of the network for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        image: NumPy array of shape (n, m, 3) with the image.\n",
    "        network: Callable function to predict the output of the network.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the output of the network.\n",
    "    \"\"\"\n",
    "    if n_classes is None:\n",
    "        n_classes = 1\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    image = np.transpose(image, (0, 3, 1, 2))\n",
    "\n",
    "    if not multi_channel:\n",
    "        image = image[:, 0:1, :, :]\n",
    "    output = network(image).reshape((-1, n_classes))\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa56162-e4c7-42a6-99f4-134b446e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(img, label, *args, **kwargs):\n",
    "    explanation = explainer_lime.explain_instance(\n",
    "        img[0, 0, :, :],\n",
    "        lambda x: batch_predict(\n",
    "            x,\n",
    "            lambda x: net(torch.from_numpy(x.astype(np.float32)).to(device)).detach().cpu().numpy(),\n",
    "            multi_channel=False,\n",
    "            n_classes=47,\n",
    "        ),\n",
    "        num_samples=1500,\n",
    "        batch_size=1,\n",
    "        random_seed=42,\n",
    "        progress_bar=False,\n",
    "        hide_color=0,\n",
    "    )\n",
    "    \n",
    "    mask = np.zeros(\n",
    "        (explanation.segments.shape[0], explanation.segments.shape[1]), dtype=np.float64,\n",
    "    )\n",
    "    \n",
    "    lime_res = []\n",
    "    for key, val in explanation.local_exp[label]:\n",
    "        if key != 0:\n",
    "            mask[explanation.segments == key] = abs(val)\n",
    "    lime_res.append(mask)\n",
    "    lime_res = np.array(lime_res)\n",
    "\n",
    "    return lime_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240c795-bb0d-4916-9773-6cf00abb9974",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ad512-3e2b-4123-8682-186bce750eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_fn = {\n",
    "    \"rise\": lambda img, pred: explainer_rise(img)[pred],\n",
    "    \"lime\": get_lime\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a92f24-8764-4b30-84af-77587b64b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for method_name, method in methods_fn.items():   \n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "    i = 0\n",
    "    for img_path in tqdm(sorted(glob(f\"./data/funny_birds/v2/test_CO/**/**/image.png\")), desc=method_name):\n",
    "        folder_path, img_name = os.path.split(img_path)\n",
    "        img_id = folder_path.split(os.path.sep)[-1]\n",
    "    \n",
    "        img_pil = Image.open(img_path)\n",
    "        img = transform(img_pil).unsqueeze(0)\n",
    "        \n",
    "        cls_prediction = int(torch.argmax(net(img.to(device))).detach().cpu().numpy())\n",
    "        explanation = method(img.to(device), cls_prediction)#.detach().cpu().numpy()\n",
    "    \n",
    "        gt = f\"./output/GT_resnet50/{img_id}.npy\"\n",
    "        gt = np.load(gt)\n",
    "\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            res = metric_fn(gt, explanation)\n",
    "            results_method[metric_name].append(res)\n",
    "        i = i +1\n",
    "        if i == 3:\n",
    "            break\n",
    "    results[method_name] = results_method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5db5b6-38cb-434e-a1e6-21e124353773",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a19ba9-b532-4c89-a7c5-0c64c664b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, method_info in results.items():\n",
    "    print(method_name)\n",
    "    for k, v in method_info.items():\n",
    "        print(f\"{k}: {np.nanmean(v)} - {np.nanstd(v)}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402299e-14c6-4551-8add-159165090a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
