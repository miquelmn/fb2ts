{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae1772-464c-45f3-9808-2e6940bacaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import rise\n",
    "import torch\n",
    "from captum import attr\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as np_rand\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import v2\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55fe08d-6f93-4136-ae12-fd5504a91830",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"./data/funny_birds/v2/test_CO/**/**/image.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c4f9f-6542-41e1-a598-eab0677c435d",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "ResNET50 with a different head. Accuracy over ~0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaefa635-d02f-499b-9f0c-1a3cda973e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = models.resnet50(num_classes=50)\n",
    "\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.7),\n",
    "    nn.Linear(128, 47),\n",
    ")\n",
    "\n",
    "\n",
    "net.load_state_dict(torch.load(\"res_9.pt\", weights_only=True))\n",
    "net = net.to(device)\n",
    "\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb7225b-3970-4734-9c58-b47d68e84fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        v2.ToDtype(torch.float),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7668fc-627f-4448-9046-d26c0567454e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get GT Relevance\n",
    "\n",
    "Obtains the relevance of each part as:\n",
    "\n",
    "$$\n",
    "\\text{Part}_{\\text{imp}} = f(x) - f(x'),\n",
    "$$\n",
    "\n",
    "where $x'$ is the same image than $x$ without the part studied.\n",
    "\n",
    "This $\\text{Part}_{\\text{imp}}$ is multiplied by the respective part. The addition of all part multiplied with their importance generates a saliency map GT, that is stored as a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29e455-1a5b-4310-9de8-368de6a3920e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = f\"./data/funny_birds/v2/test_CO/**/**/image.png\"\n",
    "\n",
    "for img_path in tqdm(sorted(glob(DATA_PATH))):\n",
    "    folder_path, img_name = os.path.split(img_path)\n",
    "    cls = folder_path.split(os.path.sep)[-2]\n",
    "    img_id = folder_path.split(os.path.sep)[-1]\n",
    "\n",
    "    original_data = Image.open(img_path)\n",
    "\n",
    "    # Prepare the data for the model\n",
    "    original_data = transform(original_data).unsqueeze(0)\n",
    "\n",
    "    output_org = net(original_data.to(device))[0][int(cls)]  # [batch][class]\n",
    "\n",
    "    parts = {\n",
    "        \"wing\": \"body_beak_eye_foot_tail.png\",\n",
    "        \"tail\": \"body_beak_eye_foot_wing.png\",\n",
    "        \"foot\": \"body_beak_eye_tail_wing.png\",\n",
    "        \"eye\": \"body_beak_foot_tail_wing.png\",\n",
    "        \"beak\": \"body_eye_foot_tail_wing.png\",\n",
    "    }\n",
    "\n",
    "    sal_map_gt = torch.zeros_like(original_data).float()\n",
    "    res = dict()\n",
    "    diff_arr = []\n",
    "    for part, part_path in parts.items():\n",
    "        path = os.path.join(folder_path, part_path)\n",
    "\n",
    "        if not os.path.isfile(path):\n",
    "            print(f\"{path} - fora\")\n",
    "\n",
    "        data = Image.open(path)\n",
    "        data = transform(data).unsqueeze(0)  # transforms.ToTensor()(data).unsqueeze(0)\n",
    "\n",
    "        output = net(data.to(device))[0][int(cls)]  # [batch][class]\n",
    "        res[part] = float((output_org - output).detach().cpu())\n",
    "\n",
    "        diff = original_data - data\n",
    "        diff = diff * res[part]\n",
    "\n",
    "        diff_arr.append(diff)\n",
    "\n",
    "        sal_map_gt = sal_map_gt + diff\n",
    "    sal_map_gt = sal_map_gt.cpu().numpy()[0, 0, :, :]\n",
    "    with open(f\"./output/GT_resnet50/{img_id}.npy\", \"wb\") as f:\n",
    "        np.save(f, sal_map_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753aa30-7d80-4665-9ce4-8d5e78913e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i, a in enumerate(diff_arr):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(abs(a.cpu().numpy()[0, 0, :, :]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d9ae7-c753-46d2-a14a-8d5d806d7dfa",
   "metadata": {},
   "source": [
    "# Get XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fae0c3f-b087-40d7-a4e8-5d678fc22d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def _to_probability(info):\n",
    "    \"\"\"Convert the input to a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        info: NumPy array with the input to convert.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the input converted to a probability distribution\n",
    "    \"\"\"\n",
    "    info = np.copy(info)\n",
    "    info_shape = info.shape\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    info = info.reshape(-1, 1)\n",
    "    info = scaler.fit_transform(info)\n",
    "    info = info.reshape(info_shape)\n",
    "\n",
    "    return info / (np.sum(info) + epsilon)\n",
    "\n",
    "\n",
    "def kl(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the Kullback-Leibler divergence between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    sal_map_gt = sal_map_gt + epsilon\n",
    "    sal_map = sal_map + epsilon\n",
    "\n",
    "    divergence = np.sum(sal_map_gt * np.log(sal_map_gt / sal_map))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "\n",
    "def sim(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the sim distance between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the min distance between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # sal_map_gt /= np.sum(sal_map_gt) + epsilon\n",
    "    # sal_map /= np.sum(sal_map) + epsilon\n",
    "\n",
    "    diff = np.min(np.stack([sal_map, sal_map_gt]), axis=0)\n",
    "    diff = np.sum(diff)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def emd(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Earth Mover's Distance between two saliency maps.\n",
    "\n",
    "    Earth Mover's Distance (EMD) is a measure of the distance between two probability distributions over a region.\n",
    "    It is defined as the minimum cost of turning one distribution into the other, where the cost is the amount of\n",
    "    \"earth\" moved, or the amount of probability mass that must be moved from one point to another.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float between 0 and 1 with the EMD between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    sal_map_gt /= sal_map_gt.max() if sal_map_gt.max() > 0 else 1\n",
    "    sal_map /= sal_map.max() if sal_map.max() > 0 else 1\n",
    "\n",
    "    diff = stats.wasserstein_distance(sal_map.flatten(), sal_map_gt.flatten())\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def _to_zero_one(info):\n",
    "    return (info - info.min()) / (info.max() - info.min())\n",
    "\n",
    "\n",
    "def AUC_Borji(sal_map_gt, sal_map, n_rep=100, step_size=0.1, rand_sampler=None):\n",
    "    \"\"\"\n",
    "    This measures how well the saliency map of an image predicts the ground truth human fixations on the image.\n",
    "    ROC curve created by sweeping through threshold values at fixed step size\n",
    "    until the maximum saliency map value.\n",
    "    True positive (tp) rate correspond to the ratio of saliency map values above threshold\n",
    "    at fixation locations to the total number of fixation locations.\n",
    "    False positive (fp) rate correspond to the ratio of saliency map values above threshold\n",
    "    at random locations to the total number of random locations\n",
    "    (as many random locations as fixations, sampled uniformly from fixation_map ALL IMAGE PIXELS),\n",
    "    averaging over n_rep number of selections of random locations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    saliency_map : real-valued matrix\n",
    "    fixation_map : binary matrix\n",
    "        Human fixation map.\n",
    "    n_rep : int, optional\n",
    "        Number of repeats for random sampling of non-fixated locations.\n",
    "    step_size : int, optional\n",
    "        Step size for sweeping through saliency map.\n",
    "    rand_sampler : callable\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "        Sample the saliency map at random locations to estimate false positive.\n",
    "        Return the sampled saliency values, S_rand.shape=(n_fix,n_rep)\n",
    "    Returns\n",
    "    -------\n",
    "    AUC : float, between [0,1]\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_zero_one(sal_map_gt)\n",
    "    sal_map = _to_zero_one(sal_map)\n",
    "\n",
    "    saliency_map = np.asarray(sal_map)\n",
    "    fixation_map = np.asarray(sal_map_gt) > 0.5\n",
    "    # If there are no fixation to predict, return NaN\n",
    "    if not np.any(fixation_map):\n",
    "        print(\"no fixation to predict\")\n",
    "        return np.nan\n",
    "    # Normalize saliency map to have values between [0,1]\n",
    "    saliency_map = _to_probability(saliency_map)\n",
    "\n",
    "    S = saliency_map.ravel()\n",
    "    F = fixation_map.ravel()\n",
    "    S_fix = S[F]  # Saliency map values at fixation locations\n",
    "    n_fix = len(S_fix)\n",
    "    n_pixels = len(S)\n",
    "    # For each fixation, sample n_rep values from anywhere on the saliency map\n",
    "    if rand_sampler is None:\n",
    "        r = np_rand.randint(0, n_pixels, [n_fix, n_rep])\n",
    "        S_rand = S[\n",
    "            r\n",
    "        ]  # Saliency map values at random locations (including fixated locations!? underestimated)\n",
    "    else:\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "    # Calculate AUC per random split (set of random locations)\n",
    "    auc = np.zeros(n_rep) * np.nan\n",
    "    for rep in range(n_rep):\n",
    "        thresholds = np.r_[0 : np.max(np.r_[S_fix, S_rand[:, rep]]) : step_size][::-1]\n",
    "        tp = np.zeros(len(thresholds) + 2)\n",
    "        fp = np.zeros(len(thresholds) + 2)\n",
    "        tp[0] = 0\n",
    "        tp[-1] = 1\n",
    "        fp[0] = 0\n",
    "        fp[-1] = 1\n",
    "        for k, thresh in enumerate(thresholds):\n",
    "            tp[k + 1] = np.sum(S_fix >= thresh) / float(n_fix)\n",
    "            fp[k + 1] = np.sum(S_rand[:, rep] >= thresh) / float(n_fix)\n",
    "        auc[rep] = np.trapz(tp, fp)\n",
    "    return np.mean(auc)  # Average across random splits\n",
    "\n",
    "\n",
    "metrics = {\"emd\": emd, \"kl\": kl, \"auc\": AUC_Borji, \"sim\": sim}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e7a1d-d761-4aa1-839e-08554e018b41",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f323f8c0-64a9-40d6-8cd1-ecf0d42b223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating filters: 100%|██████████| 600/600 [00:01<00:00, 357.96it/s]\n"
     ]
    }
   ],
   "source": [
    "explainer_rise = rise.RISE(net, (256, 256), gpu_batch=1, device=device)\n",
    "explainer_rise.generate_masks(N=600, s=8, p1=0.1, savepath=\"masks.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb77b70-50ab-4cbd-9b0a-dc8850239657",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6544582b-f248-4085-a395-7b24df0965f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lime = LimeImageExplainer()\n",
    "\n",
    "\n",
    "def batch_predict(\n",
    "    image: np.array, network, multi_channel: bool = False, n_classes=None\n",
    ") -> np.array:\n",
    "    \"\"\"Function to predict the output of the network for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        image: NumPy array of shape (n, m, 3) with the image.\n",
    "        network: Callable function to predict the output of the network.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the output of the network.\n",
    "    \"\"\"\n",
    "    if n_classes is None:\n",
    "        n_classes = 1\n",
    "\n",
    "    image = np.copy(image)\n",
    "    image = np.transpose(image, (0, 3, 1, 2))\n",
    "\n",
    "    if not multi_channel:\n",
    "        image = image[:, 0:1, :, :]\n",
    "    output = network(image).reshape((-1, n_classes))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa56162-e4c7-42a6-99f4-134b446e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(img, label, *args, **kwargs):\n",
    "    explanation = explainer_lime.explain_instance(\n",
    "        img[0, 0, :, :],\n",
    "        lambda x: batch_predict(\n",
    "            x,\n",
    "            lambda x: net(torch.from_numpy(x.astype(np.float32)).to(device))\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy(),\n",
    "            multi_channel=False,\n",
    "            n_classes=47,\n",
    "        ),\n",
    "        num_samples=1500,\n",
    "        batch_size=1,\n",
    "        random_seed=42,\n",
    "        progress_bar=False,\n",
    "        hide_color=0,\n",
    "    )\n",
    "\n",
    "    mask = np.zeros(\n",
    "        (explanation.segments.shape[0], explanation.segments.shape[1]),\n",
    "        dtype=np.float64,\n",
    "    )\n",
    "\n",
    "    lime_res = []\n",
    "    for key, val in explanation.local_exp[label]:\n",
    "        if key != 0:\n",
    "            mask[explanation.segments == key] = abs(val)\n",
    "    lime_res.append(mask)\n",
    "    lime_res = np.array(lime_res)\n",
    "\n",
    "    return lime_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95aa8e-3d2a-49b8-b7c8-49f6dba05e2a",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2b69ea-8290-4c25-b69b-ecad523e66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, y, xai):\n",
    "    res = xai.attribute(x.to(device), target=y)[:, 0, :, :]\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "sal = attr.Saliency(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e05c2-4dea-44aa-99aa-6e38ef9f8956",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c398a0a5-e70d-40a8-ab99-13f0b01d315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d132e-227c-4555-8abf-df7d8760629b",
   "metadata": {},
   "source": [
    "### IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63be23df-cbcf-4d41-a40c-6857afbc6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = attr.IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e45fe2-e138-4ff2-89a8-8083b547739c",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25d3224-a713-40a8-a29d-0e59380827d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcb3a7-9603-4ebc-9124-100e891bcd8c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248e4739-552f-45cf-9602-d788f3bbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift = attr.DeepLift(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7ad512-3e2b-4123-8682-186bce750eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_fn = {\n",
    "    \"dl\": lambda x, y: grad_fn(x, y, deep_lift),\n",
    "    \"shap\": lambda x, y: kernel_shap.attribute(x.to(device), target=y, n_samples=200),\n",
    "    \"rise\": lambda img, pred: explainer_rise(img)[pred],\n",
    "    \"lime\": get_lime,\n",
    "    \"grad\": lambda x, y: grad_fn(x, y, sal),\n",
    "    \"ig\": lambda x, y: grad_fn(x, y, ig),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a92f24-8764-4b30-84af-77587b64b5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ed304d99af460ca8adcb17f83f735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dl:   0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A Module ReLU(inplace=True) was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m img = transform(img_pil).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     15\u001b[39m cls_prediction = \u001b[38;5;28mint\u001b[39m(torch.argmax(net(img.to(device))).detach().cpu().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m explanation = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_prediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(explanation, torch.Tensor):\n\u001b[32m     19\u001b[39m     explanation = explanation.detach().cpu().numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m      1\u001b[39m methods_fn = {\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdl\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_lift\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mshap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: kernel_shap.attribute(x.to(device), target=y, n_samples=\u001b[32m200\u001b[39m),\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrise\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m img, pred: explainer_rise(img)[pred],\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlime\u001b[39m\u001b[33m\"\u001b[39m: get_lime,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: grad_fn(x, y, sal),\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mig\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: grad_fn(x, y, ig),\n\u001b[32m      8\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgrad_fn\u001b[39m\u001b[34m(x, y, xai)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrad_fn\u001b[39m(x, y, xai):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     res = \u001b[43mxai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m, :, :]\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/log/__init__.py:42\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/attr/_core/deep_lift.py:330\u001b[39m, in \u001b[36mDeepLift.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[39m\n\u001b[32m    320\u001b[39m expanded_target = _expand_target(\n\u001b[32m    321\u001b[39m     target, \u001b[32m2\u001b[39m, expansion_type=ExpansionTypes.repeat\n\u001b[32m    322\u001b[39m )\n\u001b[32m    324\u001b[39m wrapped_forward_func = \u001b[38;5;28mself\u001b[39m._construct_forward_func(\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    326\u001b[39m     (inputs, baselines),\n\u001b[32m    327\u001b[39m     expanded_target,\n\u001b[32m    328\u001b[39m     additional_forward_args,\n\u001b[32m    329\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m gradients = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_forward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m custom_attribution_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multiplies_by_inputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/_utils/gradient.py:119\u001b[39m, in \u001b[36mcompute_gradients\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[32m0\u001b[39m].numel() == \u001b[32m1\u001b[39m, (\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTarget not provided when necessary, cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m take gradient with respect to multiple outputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     grads = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/_utils/common.py:713\u001b[39m, in \u001b[36m_register_backward_hook.<locals>.pre_hook.<locals>.input_tensor_hook\u001b[39m\u001b[34m(input_grad)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grad_out) == \u001b[32m0\u001b[39m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m hook_out = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hook_out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hook_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m hook_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hdd/miquel/Projectes/interventions/.venv/lib/python3.11/site-packages/captum/attr/_core/deep_lift.py:434\u001b[39m, in \u001b[36mDeepLift._backward_hook\u001b[39m\u001b[34m(self, module, grad_input, grad_output)\u001b[39m\n\u001b[32m    432\u001b[39m attr_criteria = \u001b[38;5;28mself\u001b[39m.satisfies_attribute_criteria(module)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr_criteria:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    435\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA Module \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m was detected that does not contain some of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe input/output attributes that are required for DeepLift \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcomputations. This can occur, for example, if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour module is being used more than once in the network.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    439\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease, ensure that module is being used only once in the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    440\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnetwork.\u001b[39m\u001b[33m\"\u001b[39m.format(module)\n\u001b[32m    441\u001b[39m     )\n\u001b[32m    443\u001b[39m multipliers = SUPPORTED_NON_LINEAR[\u001b[38;5;28mtype\u001b[39m(module)](\n\u001b[32m    444\u001b[39m     module,\n\u001b[32m    445\u001b[39m     module.input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     eps=\u001b[38;5;28mself\u001b[39m.eps,\n\u001b[32m    450\u001b[39m )\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# remove all the properies that we set for the inputs and output\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: A Module ReLU(inplace=True) was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network."
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for method_name, method in methods_fn.items():\n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "    i = 0\n",
    "    for img_path in tqdm(\n",
    "        sorted(glob(f\"./data/funny_birds/v2/test_CO/**/**/image.png\")), desc=method_name\n",
    "    ):\n",
    "        folder_path, img_name = os.path.split(img_path)\n",
    "        img_id = folder_path.split(os.path.sep)[-1]\n",
    "\n",
    "        img_pil = Image.open(img_path)\n",
    "        img = transform(img_pil).unsqueeze(0)\n",
    "\n",
    "        cls_prediction = int(torch.argmax(net(img.to(device))).detach().cpu().numpy())\n",
    "        explanation = method(img.to(device), cls_prediction)\n",
    "\n",
    "        if isinstance(explanation, torch.Tensor):\n",
    "            explanation = explanation.detach().cpu().numpy()\n",
    "\n",
    "        gt = f\"./output/GT_resnet50/{img_id}.npy\"\n",
    "        gt = np.load(gt)\n",
    "\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            res = metric_fn(gt.flatten(), explanation.flatten())\n",
    "            results_method[metric_name].append(float(res))\n",
    "        i = i + 1\n",
    "    results[method_name] = results_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3114151-fc9a-4a70-b4e0-ec86682784fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"results_binary_img.json\"\n",
    "\n",
    "with open(RESULTS_PATH, \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a19ba9-b532-4c89-a7c5-0c64c664b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, method_info in results.items():\n",
    "    print(method_name.upper())\n",
    "    for k, v in method_info.items():\n",
    "        print(f\"{k}: {np.nanmean(v)} - {np.nanstd(v)}\")\n",
    "    print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36471b-7e8d-49b2-befa-d9608b23c845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv Auto-sync)",
   "language": "python",
   "name": "auto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
