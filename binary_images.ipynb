{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae1772-464c-45f3-9808-2e6940bacaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import rise\n",
    "import torch\n",
    "from captum import attr\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random as np_rand\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import v2\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fe08d-6f93-4136-ae12-fd5504a91830",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"./data/funny_birds/v2/test_CO/**/**/image.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c4f9f-6542-41e1-a598-eab0677c435d",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "ResNET50 with a different head. Accuracy over ~0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefa635-d02f-499b-9f0c-1a3cda973e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = models.resnet50(num_classes=50)\n",
    "\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.7),\n",
    "    nn.Linear(128, 47),\n",
    ")\n",
    "\n",
    "\n",
    "net.load_state_dict(torch.load(\"res_9.pt\", weights_only=True))\n",
    "net = net.to(device)\n",
    "\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7225b-3970-4734-9c58-b47d68e84fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        v2.ToDtype(torch.float),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0a869-a4d6-45b0-9c91-fdf22b516cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "gts = []\n",
    "\n",
    "for i, img_path in enumerate(\n",
    "    tqdm(sorted(glob(f\"./data/funny_birds/v2/test_CO/**/**/image.png\")))\n",
    "):\n",
    "    folder_path, img_name = os.path.split(img_path)\n",
    "    gt = int(folder_path.split(os.path.sep)[-2])\n",
    "\n",
    "    img_pil = Image.open(img_path)\n",
    "    img = transform(img_pil).unsqueeze(0)\n",
    "\n",
    "    pred = int(torch.argmax(net(img.to(device))).cpu().detach().numpy())\n",
    "\n",
    "    preds.append(pred)\n",
    "    gts.append(gt)\n",
    "\n",
    "\n",
    "print(classification_report(preds, gts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7668fc-627f-4448-9046-d26c0567454e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get GT Relevance\n",
    "\n",
    "Obtains the relevance of each part as:\n",
    "\n",
    "$$\n",
    "\\text{Part}_{\\text{imp}} = f(x) - f(x'),\n",
    "$$\n",
    "\n",
    "where $x'$ is the same image than $x$ without the part studied.\n",
    "\n",
    "This $\\text{Part}_{\\text{imp}}$ is multiplied by the respective part. The addition of all part multiplied with their importance generates a saliency map GT, that is stored as a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29e455-1a5b-4310-9de8-368de6a3920e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = f\"./data/funny_birds/v2/test_CO/**/**/image.png\"\n",
    "\n",
    "for img_path in tqdm(sorted(glob(DATA_PATH))):\n",
    "    folder_path, img_name = os.path.split(img_path)\n",
    "    cls = folder_path.split(os.path.sep)[-2]\n",
    "    img_id = folder_path.split(os.path.sep)[-1]\n",
    "\n",
    "    original_data = Image.open(img_path)\n",
    "\n",
    "    # Prepare the data for the model\n",
    "    original_data = transform(original_data).unsqueeze(0)\n",
    "\n",
    "    output_org = net(original_data.to(device))[0][int(cls)]  # [batch][class]\n",
    "\n",
    "    parts = {\n",
    "        \"wing\": \"body_beak_eye_foot_tail.png\",\n",
    "        \"tail\": \"body_beak_eye_foot_wing.png\",\n",
    "        \"foot\": \"body_beak_eye_tail_wing.png\",\n",
    "        \"eye\": \"body_beak_foot_tail_wing.png\",\n",
    "        \"beak\": \"body_eye_foot_tail_wing.png\",\n",
    "    }\n",
    "\n",
    "    sal_map_gt = torch.zeros_like(original_data).float()\n",
    "    res = dict()\n",
    "    diff_arr = []\n",
    "    for part, part_path in parts.items():\n",
    "        path = os.path.join(folder_path, part_path)\n",
    "\n",
    "        if not os.path.isfile(path):\n",
    "            print(f\"{path} - fora\")\n",
    "\n",
    "        data = Image.open(path)\n",
    "        data = transform(data).unsqueeze(0)  # transforms.ToTensor()(data).unsqueeze(0)\n",
    "\n",
    "        output = net(data.to(device))[0][int(cls)]  # [batch][class]\n",
    "        res[part] = float((output_org - output).detach().cpu())\n",
    "\n",
    "        diff = original_data - data\n",
    "        diff = diff * res[part]\n",
    "\n",
    "        diff_arr.append(diff)\n",
    "\n",
    "        sal_map_gt = sal_map_gt + diff\n",
    "    sal_map_gt = sal_map_gt.cpu().numpy()[0, 0, :, :]\n",
    "    with open(f\"./output/GT_resnet50/{img_id}.npy\", \"wb\") as f:\n",
    "        np.save(f, sal_map_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753aa30-7d80-4665-9ce4-8d5e78913e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i, a in enumerate(diff_arr):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(abs(a.cpu().numpy()[0, 0, :, :]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d9ae7-c753-46d2-a14a-8d5d806d7dfa",
   "metadata": {},
   "source": [
    "# Get XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae0c3f-b087-40d7-a4e8-5d678fc22d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def _to_probability(info):\n",
    "    \"\"\"Convert the input to a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        info: NumPy array with the input to convert.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the input converted to a probability distribution\n",
    "    \"\"\"\n",
    "    info = np.copy(info)\n",
    "    info_shape = info.shape\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    info = info.reshape(-1, 1)\n",
    "    info = scaler.fit_transform(info)\n",
    "    info = info.reshape(info_shape)\n",
    "\n",
    "    return info / (np.sum(info) + epsilon)\n",
    "\n",
    "\n",
    "def kl(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the Kullback-Leibler divergence between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    sal_map_gt = sal_map_gt + epsilon\n",
    "    sal_map = sal_map + epsilon\n",
    "\n",
    "    divergence = np.sum(sal_map_gt * np.log(sal_map_gt / sal_map))\n",
    "\n",
    "    return divergence\n",
    "\n",
    "\n",
    "def sim(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the sim distance between two saliency maps.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float with the min distance between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    diff = np.min(np.stack([sal_map, sal_map_gt]), axis=0)\n",
    "    diff = np.sum(diff)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def emd(sal_map_gt, sal_map):\n",
    "    \"\"\"Compute the Earth Mover's Distance between two saliency maps.\n",
    "\n",
    "    Earth Mover's Distance (EMD) is a measure of the distance between two probability distributions over a region.\n",
    "    It is defined as the minimum cost of turning one distribution into the other, where the cost is the amount of\n",
    "    \"earth\" moved, or the amount of probability mass that must be moved from one point to another.\n",
    "\n",
    "    Args:\n",
    "        sal_map_gt: NumPy array with the ground truth saliency map.\n",
    "        sal_map: NumPy array with the saliency map to compare.\n",
    "\n",
    "    Returns:\n",
    "        Float between 0 and 1 with the EMD between the two saliency maps.\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_probability(sal_map_gt)\n",
    "    sal_map = _to_probability(sal_map)\n",
    "\n",
    "    sal_map_gt /= sal_map_gt.max() if sal_map_gt.max() > 0 else 1\n",
    "    sal_map /= sal_map.max() if sal_map.max() > 0 else 1\n",
    "\n",
    "    diff = stats.wasserstein_distance(sal_map.flatten(), sal_map_gt.flatten())\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def _to_zero_one(info):\n",
    "    return (info - info.min()) / (info.max() - info.min())\n",
    "\n",
    "\n",
    "def AUC_Borji(sal_map_gt, sal_map, n_rep=100, step_size=0.1, rand_sampler=None):\n",
    "    \"\"\"\n",
    "    This measures how well the saliency map of an image predicts the ground truth human fixations on the image.\n",
    "    ROC curve created by sweeping through threshold values at fixed step size until the maximum saliency map value.\n",
    "\n",
    "    True positive (tp) rate correspond to the ratio of saliency map values above threshold at fixation locations\n",
    "    to the total number of fixation locations.\n",
    "\n",
    "    False positive (fp) rate correspond to the ratio of saliency map values above threshold at random locations to\n",
    "    the total number of random locations (as many random locations as fixations, sampled uniformly from fixation_map\n",
    "    ALL IMAGE PIXELS), averaging over n_rep number of selections of random locations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    saliency_map : real-valued matrix\n",
    "    fixation_map : binary matrix\n",
    "        Human fixation map.\n",
    "    n_rep : int, optional\n",
    "        Number of repeats for random sampling of non-fixated locations.\n",
    "    step_size : int, optional\n",
    "        Step size for sweeping through saliency map.\n",
    "    rand_sampler : callable\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "        Sample the saliency map at random locations to estimate false positive.\n",
    "        Return the sampled saliency values, S_rand.shape=(n_fix,n_rep)\n",
    "    Returns\n",
    "    -------\n",
    "    AUC : float, between [0,1]\n",
    "    \"\"\"\n",
    "    sal_map_gt = _to_zero_one(sal_map_gt)\n",
    "    sal_map = _to_zero_one(sal_map)\n",
    "\n",
    "    saliency_map = np.asarray(sal_map)\n",
    "    fixation_map = np.asarray(sal_map_gt) > 0.5\n",
    "    # If there are no fixation to predict, return NaN\n",
    "    if not np.any(fixation_map):\n",
    "        print(\"no fixation to predict\")\n",
    "        return np.nan\n",
    "    # Normalize saliency map to have values between [0,1]\n",
    "    # saliency_map = _to_probability(saliency_map)\n",
    "\n",
    "    S = saliency_map.ravel()\n",
    "    F = fixation_map.ravel()\n",
    "    S_fix = S[F]  # Saliency map values at fixation locations\n",
    "    n_fix = len(S_fix)\n",
    "    n_pixels = len(S)\n",
    "    # For each fixation, sample n_rep values from anywhere on the saliency map\n",
    "    if rand_sampler is None:\n",
    "        r = np_rand.randint(0, n_pixels, [n_fix, n_rep])\n",
    "        S_rand = S[\n",
    "            r\n",
    "        ]  # Saliency map values at random locations (including fixated locations!? underestimated)\n",
    "    else:\n",
    "        S_rand = rand_sampler(S, F, n_rep, n_fix)\n",
    "    # Calculate AUC per random split (set of random locations)\n",
    "    auc = np.zeros(n_rep) * np.nan\n",
    "    for rep in range(n_rep):\n",
    "        thresholds = np.r_[0 : np.max(np.r_[S_fix, S_rand[:, rep]]) : step_size][::-1]\n",
    "        tp = np.zeros(len(thresholds) + 2)\n",
    "        fp = np.zeros(len(thresholds) + 2)\n",
    "        tp[0] = 0\n",
    "        tp[-1] = 1\n",
    "        fp[0] = 0\n",
    "        fp[-1] = 1\n",
    "        for k, thresh in enumerate(thresholds):\n",
    "            tp[k + 1] = np.sum(S_fix >= thresh) / float(n_fix)\n",
    "            fp[k + 1] = np.sum(S_rand[:, rep] >= thresh) / float(n_fix)\n",
    "        auc[rep] = np.trapezoid(tp, fp)\n",
    "    return np.mean(auc)  # Average across random splits\n",
    "\n",
    "\n",
    "metrics = {\"emd\": emd, \"kl\": kl, \"sim\": sim, \"auc\": AUC_Borji}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e7a1d-d761-4aa1-839e-08554e018b41",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323f8c0-64a9-40d6-8cd1-ecf0d42b223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_rise = rise.RISE(net, (256, 256), gpu_batch=1, device=device)\n",
    "explainer_rise.generate_masks(N=600, s=8, p1=0.1, savepath=\"masks.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb77b70-50ab-4cbd-9b0a-dc8850239657",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544582b-f248-4085-a395-7b24df0965f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lime = LimeImageExplainer()\n",
    "\n",
    "\n",
    "def batch_predict(\n",
    "    image: np.array, network, multi_channel: bool = False, n_classes=None\n",
    ") -> np.array:\n",
    "    \"\"\"Function to predict the output of the network for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        image: NumPy array of shape (n, m, 3) with the image.\n",
    "        network: Callable function to predict the output of the network.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array with the output of the network.\n",
    "    \"\"\"\n",
    "    if n_classes is None:\n",
    "        n_classes = 1\n",
    "\n",
    "    image = np.copy(image)\n",
    "    image = np.transpose(image, (0, 3, 1, 2))\n",
    "\n",
    "    if not multi_channel:\n",
    "        image = image[:, 0:1, :, :]\n",
    "    output = network(image).reshape((-1, n_classes))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa56162-e4c7-42a6-99f4-134b446e67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(img, label, *args, **kwargs):\n",
    "    explanation = explainer_lime.explain_instance(\n",
    "        img[0, 0, :, :],\n",
    "        lambda x: batch_predict(\n",
    "            x,\n",
    "            lambda x: net(torch.from_numpy(x.astype(np.float32)).to(device))\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy(),\n",
    "            multi_channel=False,\n",
    "            n_classes=47,\n",
    "        ),\n",
    "        num_samples=1500,\n",
    "        batch_size=1,\n",
    "        random_seed=42,\n",
    "        progress_bar=False,\n",
    "        hide_color=0,\n",
    "    )\n",
    "\n",
    "    mask = np.zeros(\n",
    "        (explanation.segments.shape[0], explanation.segments.shape[1]),\n",
    "        dtype=np.float64,\n",
    "    )\n",
    "\n",
    "    lime_res = []\n",
    "    for key, val in explanation.local_exp[label]:\n",
    "        if key != 0:\n",
    "            mask[explanation.segments == key] = abs(val)\n",
    "    lime_res.append(mask)\n",
    "    lime_res = np.array(lime_res)\n",
    "\n",
    "    return lime_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95aa8e-3d2a-49b8-b7c8-49f6dba05e2a",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b69ea-8290-4c25-b69b-ecad523e66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(x, y, xai):\n",
    "    res = xai.attribute(x.to(device), target=y)[:, 0, :, :]\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "sal = attr.Saliency(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e05c2-4dea-44aa-99aa-6e38ef9f8956",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398a0a5-e70d-40a8-ab99-13f0b01d315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d132e-227c-4555-8abf-df7d8760629b",
   "metadata": {},
   "source": [
    "### IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be23df-cbcf-4d41-a40c-6857afbc6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = attr.IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e45fe2-e138-4ff2-89a8-8083b547739c",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d3224-a713-40a8-a29d-0e59380827d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap = attr.KernelShap(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcb3a7-9603-4ebc-9124-100e891bcd8c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09aaf9-d3fb-4137-a565-c8321da1a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"results_binary_img.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e4739-552f-45cf-9602-d788f3bbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift = attr.DeepLift(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ad512-3e2b-4123-8682-186bce750eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_fn = {\n",
    "    \"dl\": lambda x, y: grad_fn(x, y, deep_lift),\n",
    "    \"shap\": lambda x, y: kernel_shap.attribute(x.to(device), target=y, n_samples=200),\n",
    "    \"rise\": lambda img, pred: explainer_rise(img)[pred],\n",
    "    \"lime\": get_lime,\n",
    "    \"grad\": lambda x, y: grad_fn(x, y, sal),\n",
    "    \"ig\": lambda x, y: grad_fn(x, y, ig),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6e89c-09c3-46e1-9267-f9a0f8392d6b",
   "metadata": {},
   "source": [
    "### Calculate XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db8758-145a-4a5d-ab75-c908bb0b0023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for method_name, method in methods_fn.items():\n",
    "    i = 0\n",
    "    out_folder_path = os.path.join(\"results\", \"bin_img\", method_name)\n",
    "    os.makedirs(out_folder_path, exist_ok=True)\n",
    "    for i, img_path in enumerate(\n",
    "        tqdm(\n",
    "            sorted(glob(f\"./data/funny_birds/v2/test_CO/**/**/image.png\")),\n",
    "            desc=method_name,\n",
    "        )\n",
    "    ):\n",
    "        folder_path, img_name = os.path.split(img_path)\n",
    "        img_id = folder_path.split(os.path.sep)[-1]\n",
    "\n",
    "        img_pil = Image.open(img_path)\n",
    "        img = transform(img_pil).unsqueeze(0)\n",
    "\n",
    "        cls_prediction = int(torch.argmax(net(img.to(device))).detach().cpu().numpy())\n",
    "        explanation = method(img.to(device), cls_prediction)\n",
    "\n",
    "        if isinstance(explanation, torch.Tensor):\n",
    "            explanation = explanation.detach().cpu().numpy()\n",
    "\n",
    "        with open(os.path.join(out_folder_path, f\"{img_id}.npy\"), \"wb\") as f:\n",
    "            np.save(f, explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d266da4-1a55-4ee5-9f12-ca81f1e85e9f",
   "metadata": {},
   "source": [
    "### Calculate measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699b601-1cfc-4a04-b9e4-a220a692d5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "\n",
    "for method_name, method in methods_fn.items():\n",
    "    results_method = {k: [] for k in metrics.keys()}\n",
    "    out_folder_path = os.path.join(\"results\", \"bin_img\", method_name)\n",
    "\n",
    "    for pred_path, img_path in zip(\n",
    "        tqdm(sorted(glob(os.path.join(out_folder_path, f\"*.npy\"))), desc=method_name),\n",
    "        sorted(glob(f\"./data/funny_birds/v2/test_CO/**/**/image.png\")),\n",
    "    ):\n",
    "        _, image_id = os.path.split(pred_path)\n",
    "\n",
    "        gt_xai = np.load(f\"./output/GT_resnet50/{image_id}\")\n",
    "        pred_xai = np.load(pred_path)\n",
    "\n",
    "        gtai = int(os.path.split(img_path)[0].split(os.path.sep)[-2])\n",
    "\n",
    "        img_pil = Image.open(img_path)\n",
    "        img = transform(img_pil).unsqueeze(0)\n",
    "\n",
    "        pred_ai = int(torch.argmax(net(img.to(device))).cpu().detach().numpy())\n",
    "\n",
    "        if pred_ai != gtai:\n",
    "            continue\n",
    "\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            res = metric_fn(gt_xai.flatten(), pred_xai.flatten())\n",
    "            results_method[metric_name].append(float(res))\n",
    "    results[method_name] = results_method\n",
    "\n",
    "with open(RESULTS_PATH, \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a19ba9-b532-4c89-a7c5-0c64c664b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for method_name, method_info in results.items():\n",
    "    print(method_name.upper())\n",
    "    for k, v in method_info.items():\n",
    "        if k == \"emd\":\n",
    "            continue\n",
    "        print(f\"{k}: {np.nanmean(v)} - {np.nanstd(v)}\")\n",
    "    print(\"-\" * 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv Auto-sync)",
   "language": "python",
   "name": "auto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
